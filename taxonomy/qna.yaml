version: 1
domain: Red Hat Documentation
created_by: yongzhuang + aron
document_outline: |
    Getting started with Red Hat OpenShift AI Self-Managed
seed_examples:
    - context: |
          To implement a data science workflow, you must create a project. In OpenShift, a project is a Kubernetes namespace with additional annotations, and is the main way that you can manage user access to resources. A project organizes your data science work in one place and also allows you to collaborate with other developers and data scientists in your organization.
          Within a project, you can add the following functionality:
            - Connections so that you can access data without having to hardcode information like endpoints or credentials.
            - Workbenches for working with and processing data, and for developing models.
            - Deployed models so that you can test them and then integrate them into intelligent applications. Deploying a model makes it available as a service that you can access by using an API.
            - Pipelines for automating your ML workflow.
          ## Prerequisites
            - You have logged in to Red Hat OpenShift AI.
            - If you are using OpenShift AI groups, you are part of the user group or admin group (for example, rhoai-users or rhoai-admins ) in OpenShift.
            - You have the appropriate roles and permissions to create projects.
          ## Procedure
            - 1. From the OpenShift AI dashboard, select Data science projects. The Data science projects page shows a list of projects that you can access. For each userrequested project in the list, the Name column shows the project display name, the user who requested the project, and the project description.
            - 2. Click Create project.
            - 3. In the Create project dialog, update the Name field to enter a unique display name for your project.
            - 4. Optional: If you want to change the default resource name for your project, click Edit resource name.
            - The resource name is what your resource is labeled in OpenShift. Valid characters include lowercase letters, numbers, and hyphens (-). The resource name cannot exceed 30 characters, and it must start with a letter and end with a letter or number.
            Note: You cannot change the resource name after the project is created. You can edit only the display name and the description.
            - 5. Optional: In the Description field, provide a project description.
            - 6. Click Create.

          questions_and_answers:
          - questions: |
             What are the functionalities we can add within a project?
            answers: |
              These are the functionalities:
                1. Connections so that you can access data without having to hardcode information like endpoints or credentials.
                2. Workbenches for working with and processing data, and for developing models.
                3. Deployed models so that you can test them and then integrate them into intelligent applications. Deploying a model makes it available as a service that you can access by using an API.
                4. Pipelines for automating your ML workflow.
          - questions: |
             What are the prerequisites for creating a data science project?
            answers: |
              These are the prerequisites:
                1. You have logged in to Red Hat OpenShift AI.
                2. If you are using OpenShift AI groups, you are part of the user group or admin group (for example, rhoai-users or rhoai-admins ) in OpenShift.
                3. You have the appropriate roles and permissions to create projects.
          - questions: |
             What are the procedures for creating a data science project?
            answers: |
              These are the procedures:
                - 1. From the OpenShift AI dashboard, select Data science projects. The Data science projects page shows a list of projects that you can access. For each userrequested project in the list, the Name column shows the project display name, the user who requested the project, and the project description.
                - 2. Click Create project.
                - 3. In the Create project dialog, update the Name field to enter a unique display name for your project.
                - 4. Optional: If you want to change the default resource name for your project, click Edit resource name.
                - The resource name is what your resource is labeled in OpenShift. Valid characters include lowercase letters, numbers, and hyphens (-). The resource name cannot exceed 30 characters, and it must start with a letter and end with a letter or number.
                Note: You cannot change the resource name after the project is created. You can edit only the display name and the description.
                - 5. Optional: In the Description field, provide a project description.
                - 6. Click Create.
    - context: |
              A workbench is an isolated area where you can examine and work with ML models. You can also work with data and run programs, for example to prepare and clean data. While a workbench is not required if, for example, you only want to service an existing model, one is needed for most data science workflow tasks, such as writing code to process data or training a model.      
              When you create a workbench, you specify an image (an IDE, packages, and other dependencies). Supported IDEs include JupyterLab, code-server, and RStudio (Technology Preview).        
              The IDEs are based on a server-client architecture. Each IDE provides a server that runs in a container on the OpenShift cluster, while the user interface (the client) is displayed in your web browser. For example, the Jupyter notebook server runs in a container on the Red Hat OpenShift cluster. The client is the JupyterLab interface that opens in your web browser on your local computer. All of the commands that you enter in JupyterLab are executed by the notebook server. Similarly, other IDEs like code-server or RStudio Server provide a server that runs in a container on the OpenShift cluster, while the user interface is displayed in your web browser. This architecture allows you to interact through your local computer in a browser environment, while all processing occurs on the cluster. The cluster provides the benefits of larger available resources and security because the data being processed never leaves the cluster.
              In a workbench, you can also configure connections (to access external data for training models and to save models so that you can deploy them) and cluster storage (for persisting data). Workbenches within the same project can share models and data through object storage with the data science pipelines and model servers.
                          
              questions_and_answers:
              - questions: |
                 What is the primary purpose of a workbench in the context of data science workflows?
                answers: |
                  The primary purpose of a workbench in data science workflows is to provide an isolated environment where users can write and execute code, prepare and clean data, and train machine learning models. It is essential for most data science tasks that involve development and experimentation, although it is not required if the user only intends to service an existing model.
              - questions: |
                 What role does the OpenShift cluster play in the server-client architecture of IDEs used in a workbench?
                answers: |
                  In the server-client architecture of IDEs within a workbench, the OpenShift cluster hosts and runs the server component of the IDE in a container. This setup ensures that all computational tasks, such as executing commands and processing data, occur securely within the cluster. The client interface is displayed in the user's web browser, allowing interaction from the local machine without transferring data outside the cluster.
              - questions: |
                 Why is a workbench not required if you only want to service an existing model?
                answers: |
                  A workbench is not required when servicing an existing model because servicing typically involves tasks like deploying or monitoring the model, which do not demand an interactive development environment or the execution of code for data processing or model training. These simpler tasks can be handled without the computational and development features a workbench provides.

    - context: |
           ABOUT WORKBENCH IMAGES
      
           A workbench image (sometimes referred to as a notebook image) is optimized with the tools and libraries that you need for model development. You can use the provided workbench images or an OpenShift AI administrator can create custom workbench images adapted to your needs.
           To provide a consistent, stable platform for your model development, many provided workbench images contain the same version of Python. Most workbench images available on OpenShift AI are pre-built and ready for you to use immediately after OpenShift AI is installed or upgraded.
           For information about Red Hat support of workbench images and packages, see Red Hat OpenShift AI: Supported Configurations.
           The following table lists the workbench images that are installed with Red Hat OpenShift AI by default.
           If the preinstalled packages that are provided in these images are not sufficient for your use case, you have the following options:
           - Install additional libraries after launching a default image. This option is good if you want to add libraries on an ad hoc basis as you develop models. However, it can be challenging to manage the dependencies of installed libraries and your changes are not saved when the workbench restarts.
           - Create a custom image that includes the additional libraries or packages. For more information, see Creating custom workbench images. configure connections (to access external data for training models and to save models so that you can deploy them) and cluster storage (for persisting data). Workbenches within the same project can share models and data through object storage with the data science pipelines and model servers.
            
            questions_and_answers:
            - questions: |
               What is the primary purpose of a workbench image in OpenShift AI?
              answers: |
                The primary purpose of a workbench in data science workflows is to provide an isolated environment where users can write and execute code, prepare and clean data, and train machine learning models. It is essential for most data science tasks that involve development and experimentation, although it is not required if the user only intends to service an existing model.
            - questions: |
               Why might an OpenShift AI administrator choose to create a custom workbench image?
              answers: |
                An OpenShift AI administrator might choose to create a custom workbench image to include specific libraries or packages that are not part of the default images, ensuring the image better meets the unique needs of users or projects.
            - questions: |
               What is a potential downside of installing additional libraries in a default workbench image after launching it?
              answers: |
               A potential downside of installing additional libraries in a default workbench image after launching it is that the dependencies can be difficult to manage, and any changes made are not preserved when the workbench is restarted.